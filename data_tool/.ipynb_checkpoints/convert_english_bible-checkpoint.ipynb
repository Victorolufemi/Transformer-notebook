{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DIR = \"eng_bible/OEBPS/\"\n",
    "d = Path(DIR)\n",
    "DATA_DIR = \"data/\"\n",
    "toc_file = DIR + \"1001061103.xhtml\"\n",
    "books = []\n",
    "headers = []\n",
    "chapter_one_files = []\n",
    "html_files = {}\n",
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_some_symbols_with_spaces(word):\n",
    "    new_word = re.sub(\"[^A-Za-z0-9]\", \" \", word)\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This processes the english table of content file\n",
    "\"\"\"\n",
    "with open(toc_file, encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    #Get Title and remove colon\n",
    "    title = soup.title.string\n",
    "    headers.append(title)\n",
    "    #Get Old Testament and New Testament headers\n",
    "    h2_headers = soup.find_all(\"h2\")\n",
    "    for header in h2_headers:\n",
    "        headers.append(replace_some_symbols_with_spaces(header.string.lower().capitalize()))\n",
    "    #Get Content Headers\n",
    "    for i in [3, 5, 6]:\n",
    "        content_header = soup.find(attrs={\"data-pid\": i})\n",
    "        headers.append(content_header.string.lower().capitalize())\n",
    "    #Get Bible Chapters For Old Testament\n",
    "    for i in range(7, 160, 4):\n",
    "        book = soup.find(attrs={\"data-pid\": i})\n",
    "        books.append(replace_some_symbols_with_spaces(book.string))\n",
    "        chapter_one_files.append(book.a['href'].split(\".\")[0])\n",
    "    #Get Bible Chapters For New Testament\n",
    "    for i in range(168, 273, 4):\n",
    "        book = soup.find(attrs={\"data-pid\": i})\n",
    "        books.append(replace_some_symbols_with_spaces(book.string))\n",
    "        chapter_one_files.append(book.a['href'].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(DIR)\n",
    "for chapter_one_file in chapter_one_files:\n",
    "    for file in all_files:\n",
    "        if file.startswith(chapter_one_file) and file.find('extracted') == -1:\n",
    "            if chapter_one_file in html_files:\n",
    "                html_files[chapter_one_file].append(file)\n",
    "            else:\n",
    "                html_files[chapter_one_file] = [file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_html = {}\n",
    "\n",
    "for key,val in html_files.items():\n",
    "    \n",
    "    if key not in html_files.items():\n",
    "        new_html[key] = []\n",
    "        store = [None]*(len(val)+1)\n",
    "        \n",
    "        for item in val:\n",
    "            split0 = item.split(\".\")[0]\n",
    "            \n",
    "            if re.match(\"(\\w+?)-split\",split0):\n",
    "                split1 = int(split0.split(\"-\")[1].split(\"split\")[1])\n",
    "#                 print(split)\n",
    "                store[split1] = item\n",
    "                \n",
    "            else:\n",
    "                store[0] = item\n",
    "        \n",
    "        del store[1]\n",
    "        new_html[key] = store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chapter_one_file in chapter_one_files:\n",
    "    for chapter_file in new_html[chapter_one_file]:\n",
    "        with open(DIR + chapter_file, encoding=\"utf-8\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "            paragraph_elements = soup.find_all(\"p\", attrs={\"class\": re.compile(\"p\\d+\")})\n",
    "            chapter_text = \"\"\n",
    "            \n",
    "            for paragraph_element in paragraph_elements:\n",
    "                classes = paragraph_element['class']\n",
    "\n",
    "                if len(classes) > 0:\n",
    "                    strong_elements = paragraph_element.find_all(\"strong\")\n",
    "                    for strong_element in strong_elements:\n",
    "                        superscripts = strong_element.findChildren(\"sup\" , recursive=False)\n",
    "                        if len(superscripts) > 0:\n",
    "                            superscripts[0].string = \"(\" + strong_element.get_text().strip() + \") \"\n",
    "                        elif strong_element.next_sibling is not None:\n",
    "                            if strong_element.next_sibling['id'].startswith('footnote'):\n",
    "                                strong_element.string = \"(\" + strong_element.get_text().strip() + \") \"\n",
    "                    chapter_text += (paragraph_element.get_text().strip() + \" \")\n",
    "            sentences.append(chapter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(unclean_string): \n",
    "    unclean_string = unclean_string.replace(\"*\", \"\")\n",
    "    iterable_matches = re.finditer('\\[\\w+\\]', unclean_string)\n",
    "    square_spans = []\n",
    "    for match in iterable_matches:\n",
    "        square_spans.append(match.span())\n",
    "    clean_string = \"\"\n",
    "    start = 0\n",
    "    for span in square_spans:\n",
    "        clean_string += unclean_string[start:span[0] - 2]\n",
    "        start = span[1]\n",
    "    clean_string += unclean_string[square_spans[-1][1]:]\n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR+\"english-chapters.txt\", \"w+\", encoding=\"utf-8\") as english_file:\n",
    "    unclean_string = \"\\n\".join(sentences)\n",
    "    clean_string = clean_string(unclean_string)\n",
    "    english_file.write(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
